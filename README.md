# [우아한 테크 코스 4-5주차 오픈미션] 기억의 지도(Map-of-Memory) 프로젝트 재도전

“MVP 우선, 고도화를 추가로”

지난 6월 27~29일 Us-Code 해커톤의 ‘잘 만들어진 빈 그릇’이라는 실패를 극복하기 위한 고난도 문제 해결 기록입니다.

## 1. 미션 재정의: “진짜 고난도 문제”

지난 6월 해커톤에서 저는 GCP 기반의 CI/CD 파이프라인 구축에 성공했지만, 정작 사용자에게 제공할 가치인 핵심 API를 구현하는 데 실패했습니다. 이것이 바로 ‘잘 만들어진 빈 그릇’입니다.

이번 재도전의 1차 목표는 이 ‘실패’를 정면으로 극복하는 것입니다. 다만, 현재 상황을 고려할 때 GCP 비용 문제라는 현실적인 제약이 생겼습니다.

**이로써 이번 미션의 ‘고난도 문제’는 명확해졌습니다.**

GCP, 즉 인프라 라는 변수가 사라졌고, 정해진 시간 안에 ‘동작하는 MVP’를 구현하고, 서비스를 고도화 하는 것이 미션의 핵심 과제입니다.

따라서 본 미션은 2단계로 진행합니다.

- 1단계 (완료): 지난 번 실패했던 내용물인 MVP를 로컬 환경에서 완벽하고 구현하고 테스트합니다.
- 2단계 (완료): 1단계를 꼭 성공한 후에, ‘기술적 아쉬움’을 해결하기 위한 고도화를 적용하고 다시 테스트합니다.

## 2. 미션 가드레일

### 반드시 해낼 것

- 핵심 MVP 도메인: `Member`, `Memory`, `Like`
- 핵심 MVP API
    - `POST /members`: 회원 등록
    - `GET /members/{memberId}`: 회원 정보 조회
    - `POST /memories?memberId={memberId}`: 기억 작성
    - `GET /memories/{memoryId}`: 기억 단건 조회
    - `GET /memories?memberId={memberId}`: 기억 리스트(페이지네이션) 조회
    - `GET /memories/map?lat=${lat}&lng=${lng}&range=${range}`: 지도 기반 기억 조회
    - `PUT /memories/{memoryId}?memberId={memberId}`: 기억 수정
    - `DELETE /memories/{memoryId}?memberId={memberId}`: 기억 삭제
    - `POST /memories/{memoryId}/like?memberId={memberId}`: 좋아요
    - `DELETE /memories/{id}/like?memberId={memberId}`: 좋아요 취소

- 로컬 개발 환경 구축
- 객체지향적 설계
- 테스트 코드

### 의식적으로 포기할 것

- CI/CD 및 클라우드 배포
- 이미지 및 파일 업로드 (Storage)
- 1단계에서의 모든 고도화 시도

### 고민해볼 것

- 사용자 인증/인가 (Spring Security)

## 3. 진행 계획

| 1주차: MVP First |    | 내용                                     | 완료 여부 |
|----------------|----|----------------------------------------|-------|
| 11/06 (1일차)    | 기획 | 미션 재정의, 제약 조건 확정 (README 작성)           | [x]   |
| 11/07 (2일차)    | 환경 | docker-compose 설정 및 환경 설정 파일 작성        | [x]   |
| 11/08 (3일차)    | 구현 | 도메인 설계 및 구현                            | [x]   |
| 11/09 (4일차)    | 구현 | 1. 핵심 API 구현(DTO, Service) 및 단위 테스트 작성 | [x]   |
| 11/10 (5일차)    | 구현 | 2. 핵심 API 구현(DTO, Service) 및 단위 테스트 작성 | [x]   |
| 11/11 (6일차)    | 검증 | JUnit 통합 테스트 작성 (API 동작 검증)            | [x]   |
| 11/12 (7일차)    | 검증 | Swagger UI / APIDog 통한 로컬 최종 검증        | [x]   |
| **2주차: 고도화**   |    |                                        |       |
| 11/13 (8일차)    | 선정 | 성능 측정 및 고도화 주제 선정(캐싱, CQRS등)           | [x]   |
| 11/14 (9일차)    | 적용 | 1. 고도화 기술 적용                           | [x]   |
| 11/15 (10일차)   | 적용 | 2. 고도화 기술 적용                           | [x]   |
| 11/16 (11일차)   | 측정 | 성능 테스트를 통한 **적용 전/후 수치 비교**            | [x]   |
| 11/17 (12일차)   | 정리 | 최종 회고 및 결과물 정리(MVP, 고도화 분석 문서)         | [x]   |

## 4. 성능 고도화 정리

본격적인 고도화(캐싱, 인덱싱) 적용에 앞서, 네 가지 시나리오를 통해 현재 시스템(Baseline)의 성능을 다각도로 분석했습니다.

### 4.1 테스트 환경

- Data Set: Member 10명, Memory 10,000건 (서울/경기 지역 랜덤 분포)
- Infrastructure: Docker (Spring Boot + MySQL + Redis + k6 + InfluxDB + Grafana)
- Tool: k6 (Load Testing)

---

### 4.2 시나리오별 상세 측정 결과

**시나리오 1: 단건 조회(k6-read-single)**

- 목적: GET /memories/{memoryId} (PK 조회) 성능 측정 -> 캐싱 필요성 진단

| 항목         |    | 측정값     | 분석                                                      |
|------------|----|---------|---------------------------------------------------------|
| 평균 응답(avg) |    | 19.04ms | 최종 Baseline 확정. 네트워크 오버헤드를 제외한 DB 쿼리 시간이 극히 짧음.         |
| 최소 응답(min) |    | 7.12ms  | 시스템의 최소 지연 시간 (네트워크 지연) 확인.                             |
| 최대 응답(max) |    | 31.2ms  | 최대 응답 시간이 평균과 매우 근접하여 응답 속도가 매우 안정적임을 확인.               |
| p95 응답 속도  |    | 25.55ms | 95%의 사용자가 0.025초 이내 응답을 받음. 캐싱 적용 시 5ms 미만 달성 가능성을 높여줌. |
| TPS        |    | ~9.78/s | 10 VUs 환경에서 준수한 처리량.                                    |

- 사진:

<img width="1838" height="1401" alt="k6-read-single-test" src="https://github.com/user-attachments/assets/d64d611d-228b-4c37-8da3-c7119a430a35" />

---

**시나리오 2: 목록 조회(k6-read-list)**

- 목적: GET /memories?memberId={memberId} (FK 조회 + 페이징) 성능 측정

| 항목         |    | 측정값     | 분석                                                                                      |
|------------|----|---------|-----------------------------------------------------------------------------------------|
| 평균 응답(avg) |    | 22.34ms | 단건 조회(19.04ms)보다 아주 약간 느리지만, 여전히 네트워크 오버헤드 수준에서 처리됨.                                    |
| 최소 응답(min) |    | 8.83ms  |                                                                                         |
| 최대 응답(max) |    | 34.6ms  |                                                                                         |
| p95 응답 속도  |    | 32.13ms | 95% 사용자가 0.032초의 응답. 외래 키(FK) 자동 인덱스 및 페이징(LIMIT 10) 덕분에 10,000건 데이터에서도 매우 안정적인 성능을 보임. |
| TPS        |    | ~9.74/s | 10 VUs 환경에서 준수한 처리량.                                                                    |

- 사진:

<img width="1838" height="1401" alt="k6-read-list-test" src="https://github.com/user-attachments/assets/d7e16390-bad1-46f3-aaab-3d4667b77963" />

---

**시나리오 3: 지도 범위 조회(k6-read-map)**

- 목적: GET /memories/map?lat={lat}&lng={lng}&range={range} (인덱스 X, Full Table Scan 유도) 성능 측정

| 항목         |    | 측정값      | 분석                                                       |
|------------|----|----------|----------------------------------------------------------|
| 평균 응답(avg) |    | 47.04ms  | 이전 FK 조회(22.34ms) 대비 2배 이상 느려짐 (Full Scan 발생 확인).        |
| 최소 응답(min) |    | 19.47ms  |                                                          |
| 최대 응답(max) |    | 268.75ms | 쿼리 부하로 인해 응답 시간이 튀는(Spike) 현상 확인. 시스템 불안정 시작.            |
| p95 응답 속도  |    | 69.37ms  | 95% 사용자가 0.069초의 응답을 받음. 약 70ms를 30ms 이하로 줄여야 합니다.       |
| TPS        |    | 9.53/s   | 응답 시간 증가에도 불구하고 처리량은 유지되었으나, 이는 DB가 과부하를 받았다는 신호임.       |
| 데이터 수신량    |    | 50MB     | (별도 측정) 한 번의 요청으로 50MB의 데이터를 전송. 네트워크 및 JSON 직렬화에 부하 가중. |

- 사진:

<img width="1838" height="1401" alt="k6-read-map-test" src="https://github.com/user-attachments/assets/654e5dca-b21a-448e-9585-1c0fca9ed62d" />

---

**시나리오 4: 스트레스 테스트(k6-stress)**

- 목적: 지도 범위 조회 API에 극한의 부하를 주어 시스템의 임계점과 병목 현상을 파악합니다.

| 항목         |    | 측정값       | 분석                                                              |
|------------|----|-----------|-----------------------------------------------------------------|
| 평균 응답(avg) |    | 2.52s     | 테스트 기간 전체의 평균 응답 시간. 이 수치가 1초를 넘었다는 것은 서비스가 이미 심각한 지연 상태였음을 의미. |
| 최소 응답(min) |    | 9.21ms    | 부하가 없을 때의 최소 응답 시간 (초기 웜업 구간).                                  |
| 최대 응답(max) |    | 8.46s     | 한 요청에 8.46초가 소요됨. 이는 요청이 Tomcat 스레드 큐에서 오랫동안 대기했음을 의미.          |
| p95 응답 속도  |    | 7.25s     | 95%의 사용자가 7.25초를 기다렸으며, 설정 목표(p95 < 500ms)를 달성하는 데 명백히 실패함.     |
| TPS        |    | ~191.89/s | 최대 2,000명의 VU가 접속했음에도 불구하고, 시스템은 초당 192개의 요청만 처리하는 한계에 도달함.     |

- 사진:

<img width="1994" height="1401" alt="k6-stress-test" src="https://github.com/user-attachments/assets/a2ab1b9a-9e1d-42b3-a514-1f4d600e3f4d" />

---

### 4.3 상세 분석 및 고도화 전략

- 측정된 데이터는 현재 MVP가 '느린 쿼리'와 '과부하'에 매우 취약함을 명확히 증명했습니다. 확보된 데이터를 기반으로 다음과 같은 2-Track 고도화 전략을 수립합니다.

---

**전략 A: 복합 인덱스 (Composite Index) 적용: 우선순위 1**

- 이 전략은 서비스 안정성을 확보하고 **심각한 병목(7.25s)**을 해소하는 것을 목표로 합니다.

| 측정                          |  | 지연 원인                       | 개선 목표             |
|-----------------------------|--|-----------------------------|-------------------|
| Read Map (p95): 69.37 ms    |  | Full Table Scan (인덱스 없음)    | < 30 ms           |
| Stress Test (p95): 7,250 ms |  | DB CPU 및 Connection Pool 포화 | < 500 ms (목표치 준수) |

**실행 계획**

1. MemoryRepository의 findAllByLatitudeBetweenAndLongitudeBetween 쿼리를 최적화합니다.
2. memories 테이블의 latitude와 longitude 컬럼에 복합 인덱스를 생성합니다.
3. 목표는 DB가 10,000건을 다 뒤지는 대신, 인덱스 트리를 통해 스캔 범위를 극도로 좁혀 쿼리 실행 시간을 획기적으로 줄이는 것입니다.

---

**전략 B: Global Cache (Redis) 도입: 우선순위 2**

- 이 전략은 **처리량(Throughput)**을 극대화하고 DB 부하를 제거하는 것을 목표로 합니다.

| 측정                        |  | 지연 원인 | 개선 목표 |
|---------------------------|--|-------|-------|
| Read List (p95): 32.13 ms |  |       |       |

**실행 계획**

1. Read Single API (GET /memories/{id})를 중심으로 캐싱을 적용합니다.
2. Redis 컨테이너를 인프라에 추가하고 Spring Data Redis를 연동합니다.
3. @Cacheable을 사용하여 DB I/O를 0으로 만들고, 응답 속도를 1ms~5ms (네트워크 지연) 수준으로 단축하며, 분산 환경에서도 데이터 정합성을 유지하는 캐싱 구조를 구축합니다.

---

### 4.4 고도화 결과

---

**전략 A: 복합 인덱스 (Composite Index) 적용**

- 지도 범위 조회 API의 성능 병목(p95: 70ms, Max: 268ms)을 해결하기 위해 복합 인덱스를 적용했습니다. 특히, MySQL 옵티마이저가 인덱스를 무시하는 문제를 FORCE INDEX 힌트를
  사용하여 강제로 해결함으로써 쿼리 성능을 획기적으로 개선하고 서비스의 안정성을 확보했습니다.

| 항목         |   | Baseline(Full Table Scan) | Index 적용 후(Native Query) | 개선율      | 분석                                              |
|------------|---|---------------------------|--------------------------|----------|-------------------------------------------------|
| p95 응답 속도  |   | 69.37ms                   | 43.29ms                  | 약 38% 개선 | 쿼리 실행 시간이 크게 단축되어 FTS 병목이 해소됨.                  |
| 최대 응답(max) |   | 268.75ms                  | 50.26 ms                 | 81% 개선   | 부하 발생 시 응답 시간이 튀는 Spike 현상이 완전히 사라져 시스템 안정성 확보. |

**기술적 상세 분석 및 해결 과정**

1. 문제 진단: Liquibase로 idx_memory_location (latitude, longitude) 복합 인덱스를 생성했으나, EXPLAIN 결과 key가 <null>로 확인되었습니다. 이는 MySQL
   옵티마이저가 **두 컬럼에 모두 범위 조건(BETWEEN)**이 걸린 쿼리(더블 Range Search)에 대해 B-tree 인덱스 사용을 비효율적이라고 판단하고 **Full Table Scan(FTS)**을
   고집했기 때문입니다.
2. 고도화 적용: 옵티마이저의 판단을 무시하고, 인덱스를 강제 사용하도록 Native Query로 전환하고 FORCE INDEX (idx_memory_location) 힌트를 적용했습니다.
3. 결과: 인덱스 강제 적용을 통해 DB의 쿼리 실행 시간이 최적화되었으며, 최대 응답 시간 기준 81% 개선이라는 대성공을 거두었습니다.

- 사진:

<img width="1641" height="1401" alt="A 진행 후 k6-read-map" src="https://github.com/user-attachments/assets/0babc9b4-74d3-4245-b678-c60b7314395e" />

---

**전략 B: Global Cache (Redis) 도입**

- 단건 조회 API의 응답 속도(p95: 27ms)를 5ms 미만으로 단축하고, DB 부하를 제거하여 분산 환경에서도 안정적인 처리량을 확보하는 것을 목표로 **Global Cache (Redis)**를
  도입했습니다.

| 항목         |   | Baseline(DB 조회) | Redis 적용 후 | 개선율      | 분석                                                                            |
|------------|---|-----------------|------------|----------|-------------------------------------------------------------------------------|
| p95 응답 속도  |   | 25.55ms         | 20.25ms    | 약 20% 개선 | DB I/O 부하가 완전히 제거됨. 안정적인 응답 속도 확보.                                            |
| 평균 응답(avg) |   | 19.04ms         | 14.89ms    | 약 21% 개선 | 캐시 미스 없이 DB 조회 비용이 사라져 평균 속도가 향상됨.                                            |
| 최소 응답(min) |   | 7.12ms          | 5.61 ms    | 약 21% 개선 | 순수 네트워크 Latency Floor 확인. 이 5.61ms는 Redis 통신, JSON 직렬화 및 Docker 네트워크 비용이 차지함. |

**기술적 상세 분석 및 해결 과정**

1. 문제 진단 및 피벗: @Cacheable 어노테이션 사용 시, JPA Entity의 복잡한 프록시 객체 직렬화(Serialization) 문제로 인해 500 Internal Server Error가 발생하며
   캐시가 실패했습니다.
2. 고도화 적용: 문제 해결을 위해 선언적 @Cacheable을 포기하고, RedisTemplate을 직접 주입받아 Cache-Aside 패턴을 프로그래밍 방식으로 구현했습니다. 이로써 AOP 프록시 직렬화 문제를
   우회하고 캐시의 조회/저장 책임을 서비스 계층에 명시했습니다.
3. 결과: 응답 속도를 25.55ms에서 20.25ms로 단축했습니다. 다만, 5ms 미만 목표는 **Redis 네트워크 통신 비용(Latency Floor)**과 JSON 직렬화/역직렬화 비용 때문에 달성하지
   못했습니다. (21ms는 현재 시스템의 물리적 한계점)

- 사진:

<img width="1641" height="1401" alt="B 진행 후 k6-read-single" src="https://github.com/user-attachments/assets/ffd64473-24cd-4db6-9fa8-6d733501b6e8" />

---
