<img width="1838" height="1401" alt="k6-read-list-test" src="https://github.com/user-attachments/assets/0058e440-e0be-4887-87fa-374d95c1331f" /># [우아한 테크 코스 4-5주차 오픈미션] 기억의 지도(Map-of-Memory) 프로젝트 재도전

“MVP 우선, 고도화를 추가로”

지난 6월 27~29일 Us-Code 해커톤의 ‘잘 만들어진 빈 그릇’이라는 실패를 극복하기 위한 고난도 문제 해결 기록입니다.

## 1. 미션 재정의: “진짜 고난도 문제”

지난 6월 해커톤에서 저는 GCP 기반의 CI/CD 파이프라인 구축에 성공했지만, 정작 사용자에게 제공할 가치인 핵심 API를 구현하는 데 실패했습니다. 이것이 바로 ‘잘 만들어진 빈 그릇’입니다.

이번 재도전의 1차 목표는 이 ‘실패’를 정면으로 극복하는 것입니다. 다만, 현재 상황을 고려할 때 GCP 비용 문제라는 현실적인 제약이 생겼습니다.

**이로써 이번 미션의 ‘고난도 문제’는 명확해졌습니다.**

GCP, 즉 인프라 라는 변수가 사라졌고, 정해진 시간 안에 ‘동작하는 MVP’를 구현하고, 서비스를 고도화 하는 것이 미션의 핵심 과제입니다.

따라서 본 미션은 2단계로 진행합니다.

- 1단계 (완료): 지난 번 실패했던 내용물인 MVP를 로컬 환경에서 완벽하고 구현하고 테스트합니다.
- 2단계 (진행 중): 1단계를 꼭 성공한 후에, ‘기술적 아쉬움’을 해결하기 위한 고도화를 적용하고 다시 테스트합니다.

## 2. 미션 가드레일

### 반드시 해낼 것

- 핵심 MVP 도메인: `Member`, `Memory`, `Like`
- 핵심 MVP API
    - `POST /members`: 회원 등록
    - `GET /members/{memberId}`: 회원 정보 조회
    - `POST /memories?memberId={memberId}`: 기억 작성
    - `GET /memories/{memoryId}`: 기억 단건 조회
    - `GET /memories?memberId={memberId}`: 기억 리스트(페이지네이션) 조회
    - `GET /memories/map?lat=${lat}&lng=${lng}&range=${range}`: 지도 기반 기억 조회
    - `PUT /memories/{memoryId}?memberId={memberId}`: 기억 수정
    - `DELETE /memories/{memoryId}?memberId={memberId}`: 기억 삭제
    - `POST /memories/{memoryId}/like?memberId={memberId}`: 좋아요
    - `DELETE /memories/{id}/like?memberId={memberId}`: 좋아요 취소

- 로컬 개발 환경 구축
- 객체지향적 설계
- 테스트 코드

### 의식적으로 포기할 것

- CI/CD 및 클라우드 배포
- 이미지 및 파일 업로드 (Storage)
- 1단계에서의 모든 고도화 시도

### 고민해볼 것

- 사용자 인증/인가 (Spring Security)

## 3. 진행 계획

| 1주차: MVP First |    | 내용                                     | 완료 여부 |
|----------------|----|----------------------------------------|-------|
| 11/06 (1일차)    | 기획 | 미션 재정의, 제약 조건 확정 (README 작성)           | [x]   |
| 11/07 (2일차)    | 환경 | docker-compose 설정 및 환경 설정 파일 작성        | [x]   |
| 11/08 (3일차)    | 구현 | 도메인 설계 및 구현                            | [x]   |
| 11/09 (4일차)    | 구현 | 1. 핵심 API 구현(DTO, Service) 및 단위 테스트 작성 | [x]   |
| 11/10 (5일차)    | 구현 | 2. 핵심 API 구현(DTO, Service) 및 단위 테스트 작성 | [x]   |
| 11/11 (6일차)    | 검증 | JUnit 통합 테스트 작성 (API 동작 검증)            | [x]   |
| 11/12 (7일차)    | 검증 | Swagger UI / APIDog 통한 로컬 최종 검증        | [x]   |
| **2주차: 고도화**   |    |                                        |       |
| 11/13 (8일차)    | 선정 | 성능 측정 및 고도화 주제 선정(캐싱, CQRS등)           | [x]   |
| 11/14 (9일차)    | 적용 | 1. 고도화 기술 적용                           | [ ]   |
| 11/15 (10일차)   | 적용 | 2. 고도화 기술 적용                           | [ ]   |
| 11/16 (11일차)   | 측정 | 성능 테스트를 통한 **적용 전/후 수치 비교**            | [ ]   |
| 11/17 (12일차)   | 정리 | 최종 회고 및 결과물 정리(MVP, 고도화 분석 문서)         | [ ]   |

## 4. 성능 고도화 정리

본격적인 고도화(캐싱, 인덱싱) 적용에 앞서, 네 가지 시나리오를 통해 현재 시스템(Baseline)의 성능을 다각도로 분석했습니다.

### 4.1 테스트 환경

- Data Set: Member 10명, Memory 10,010건 (서울/경기 지역 랜덤 분포)
- Infrastructure: Docker (Spring Boot + MySQL + Redis + k6 + InfluxDB + Grafana)
- Tool: k6 (Load Testing)

---

### 4.2 시나리오별 상세 측정 결과

**시나리오 1: 단건 조회(k6-read-single)**

- 목적: GET /memories/{memoryId} (PK 조회) 성능 측정 -> 캐싱 필요성 진단

| 항목         |    | 측정값     | 분석                                                      |
|------------|----|---------|---------------------------------------------------------|
| 평균 응답(avg) |    | 19.04ms | 최종 Baseline 확정. 네트워크 오버헤드를 제외한 DB 쿼리 시간이 극히 짧음.         |
| 최소 응답(min) |    | 7.12ms  | 시스템의 최소 지연 시간 (네트워크 지연) 확인.                             |
| 최대 응답(max) |    | 31.2ms  | 최대 응답 시간이 평균과 매우 근접하여 응답 속도가 매우 안정적임을 확인.               |
| p95 응답 속도  |    | 25.55ms | 95%의 사용자가 0.025초 이내 응답을 받음. 캐싱 적용 시 5ms 미만 달성 가능성을 높여줌. |
| TPS        |    | ~9.78/s | 10 VUs 환경에서 준수한 처리량.                                    |

- 사진:

<img width="1838" height="1401" alt="k6-read-single-test" src="https://github.com/user-attachments/assets/d64d611d-228b-4c37-8da3-c7119a430a35" />

---

**시나리오 2: 목록 조회 (Read List)**

- 목적: GET /memories?memberId={memberId} (FK 조회 + 페이징) 성능 측정

| 항목         |    | 측정값     | 분석                                                                                      |
|------------|----|---------|-----------------------------------------------------------------------------------------|
| 평균 응답(avg) |    | 22.34ms | 단건 조회(19.04ms)보다 아주 약간 느리지만, 여전히 네트워크 오버헤드 수준에서 처리됨.                                    |
| 최소 응답(min) |    | 8.83ms  |                                                                                         |
| 최대 응답(max) |    | 34.6ms  |                                                                                         |
| p95 응답 속도  |    | 32.13ms | 95% 사용자가 0.032초의 응답. 외래 키(FK) 자동 인덱스 및 페이징(LIMIT 10) 덕분에 10,000건 데이터에서도 매우 안정적인 성능을 보임. |
| TPS        |    | ~9.74/s | 10 VUs 환경에서 준수한 처리량.                                                                    |

- 사진:

<img width="1838" height="1401" alt="k6-read-list-test" src="https://github.com/user-attachments/assets/d7e16390-bad1-46f3-aaab-3d4667b77963" />

---

**시나리오 3: 지도 범위 조회 (Read Map)**

- 목적: GET /memories/map?lat={lat}&lng={lng}&range={range} (인덱스 X, Full Table Scan 유도) 성능 측정

| 항목         |    | 측정값      | 분석                                                       |
|------------|----|----------|----------------------------------------------------------|
| 평균 응답(avg) |    | 47.04ms  | 이전 FK 조회(22.34ms) 대비 2배 이상 느려짐 (Full Scan 발생 확인).        |
| 최소 응답(min) |    | 19.47ms  |                                                          |
| 최대 응답(max) |    | 268.75ms | 쿼리 부하로 인해 응답 시간이 튀는(Spike) 현상 확인. 시스템 불안정 시작.            |
| p95 응답 속도  |    | 69.37ms  | 95% 사용자가 0.069초의 응답을 받음. 약 70ms를 30ms 이하로 줄여야 합니다.       |
| TPS        |    | 9.53/s   | 응답 시간 증가에도 불구하고 처리량은 유지되었으나, 이는 DB가 과부하를 받았다는 신호임.       |
| 데이터 수신량    |    | 50MB     | (별도 측정) 한 번의 요청으로 50MB의 데이터를 전송. 네트워크 및 JSON 직렬화에 부하 가중. |

- 사진:

<img width="1838" height="1401" alt="k6-read-map-test" src="https://github.com/user-attachments/assets/654e5dca-b21a-448e-9585-1c0fca9ed62d" />

---

**시나리오 4: 스트레스 테스트 (Stress)**

- 목적: 지도 범위 조회 API에 극한의 부하를 주어 시스템의 임계점과 병목 현상을 파악합니다.

| 항목         |    | 측정값       | 분석                                                              |
|------------|----|-----------|-----------------------------------------------------------------|
| 평균 응답(avg) |    | 2.52s     | 테스트 기간 전체의 평균 응답 시간. 이 수치가 1초를 넘었다는 것은 서비스가 이미 심각한 지연 상태였음을 의미. |
| 최소 응답(min) |    | 9.21ms    | 부하가 없을 때의 최소 응답 시간 (초기 웜업 구간).                                  |
| 최대 응답(max) |    | 8.46s     | 한 요청에 8.46초가 소요됨. 이는 요청이 Tomcat 스레드 큐에서 오랫동안 대기했음을 의미.          |
| p95 응답 속도  |    | 7.25s     | 95%의 사용자가 7.25초를 기다렸으며, 설정 목표(p95 < 500ms)를 달성하는 데 명백히 실패함.     |
| TPS        |    | ~191.89/s | 최대 2,000명의 VU가 접속했음에도 불구하고, 시스템은 초당 192개의 요청만 처리하는 한계에 도달함.     |

- 사진:

<img width="1994" height="1401" alt="k6-stress-test" src="https://github.com/user-attachments/assets/a2ab1b9a-9e1d-42b3-a514-1f4d600e3f4d" />

---

### 4.3 상세 분석 및 고도화 전략

- 측정된 데이터는 현재 MVP가 '느린 쿼리'와 '과부하'에 매우 취약함을 명확히 증명했습니다. 확보된 데이터를 기반으로 다음과 같은 2-Track 고도화 전략을 수립합니다.

---

**전략 A: 복합 인덱스 (Composite Index) 적용: 우선순위 1**

- 이 전략은 서비스 안정성을 확보하고 **심각한 병목(7.25s)**을 해소하는 것을 목표로 합니다.

| 측정                          |  | 지연 원인                       | 개선 목표             |
|-----------------------------|--|-----------------------------|-------------------|
| Read Map (p95): 69.37 ms    |  | Full Table Scan (인덱스 없음)    | < 30 ms           |
| Stress Test (p95): 7,250 ms |  | DB CPU 및 Connection Pool 포화 | < 500 ms (목표치 준수) |

**실행 계획**

1. MemoryRepository의 findAllByLatitudeBetweenAndLongitudeBetween 쿼리를 최적화합니다.
2. memories 테이블의 latitude와 longitude 컬럼에 복합 인덱스를 생성합니다.
3. 목표는 DB가 10,000건을 다 뒤지는 대신, 인덱스 트리를 통해 스캔 범위를 극도로 좁혀 쿼리 실행 시간을 획기적으로 줄이는 것입니다.

---

**전략 B: Global Cache (Redis) 도입: 우선순위 2**

- 이 전략은 **처리량(Throughput)**을 극대화하고 DB 부하를 제거하는 것을 목표로 합니다.

| 측정                        |  | 지연 원인 | 개선 목표 |
|---------------------------|--|-------|-------|
| Read List (p95): 32.13 ms |  |       |       |

**실행 계획**
실행 계획 (Day 10):

1. Read Single API (GET /memories/{id})를 중심으로 캐싱을 적용합니다.
2. Redis 컨테이너를 인프라에 추가하고 Spring Data Redis를 연동합니다.
3. @Cacheable을 사용하여 DB I/O를 0으로 만들고, 응답 속도를 1ms~5ms (네트워크 지연) 수준으로 단축하며, 분산 환경에서도 데이터 정합성을 유지하는 캐싱 구조를 구축합니다.
